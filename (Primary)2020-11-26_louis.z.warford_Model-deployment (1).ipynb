{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several things of note:\n",
    "* The task is in the form of a supervised problem.\n",
    "* Initially there are 2 target values to an equal or larger range of feature values.\n",
    "    * Target values at face value, are a 1x2 array, whereas features will depend on our selection of time intervals and will be of the form Nx6, where N is $\\frac{\\Delta t}{\\delta t}$; $\\Delta T$, how far back in time before the 'present' we take values from, and $\\delta t$ the time interval length (1 min. in our case).\n",
    "        * This means that we have a hyperparameter $\\Delta T$ that we need to decide on how we tune.\n",
    "            * The most apparent ways we can do this are:\n",
    "                1. We simply take the best performing overall value for it; this being after fitting for each potential value and potential data set and comparing. (i.e. Grid search)\n",
    "                2. We use another supervised model to predict the best performing hyper-parameter given the feature data. \n",
    "                    * This could also involve deciding the parameters of a weighting function that gives a weight as a function of $n \\cdot \\delta t$, where n is the interval number.  \n",
    "                    * The reason to do this would be that potentially the current conditions will determine how far in the past  and how much predictive power the data holds. (e.g. A high current speed may mean that only very recent data is relevant.)\n",
    "                3. We take an arbitrary number we decide via some intuitive reason.\n",
    "            \n",
    "            * This will likely be unnecessary for the Neural networks as they will likely account for the weighting if we feed it the entire intervals data.\n",
    "\n",
    "        * We can take this array from the larger array of predicted values that most models would output.\n",
    "            * This allows us to use the lag approach.\n",
    "            \n",
    "\n",
    "* Potential models we will try to work with are:\n",
    "    * Various linear models:\n",
    "        * These will be basic, but they could potentially be a good fit.\n",
    "        \n",
    "    * Random Forest regressor\n",
    "        * Values are constrained in a range so the predicting power of a rfr may be very good.\n",
    "        \n",
    "    * Neural networks:\n",
    "        * Artificial Neural network.\n",
    "        * Recurrent Neural network.\n",
    "        * Potentially deep variants of the above.\n",
    "    \n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by loading in the data and formating it (since we know what the data looks like) into usable objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.843532336863314, 1030.5901079882124, 2935.660276152265,\n",
       "        38.64101791, 5488.175539727612, 11.737356515341975],\n",
       "       [7.681607109179845, 1039.8698466702335, 2928.333772398626,\n",
       "        45.24365616, 5423.93012617854, 11.543755166261553],\n",
       "       [7.514173279340541, 1033.2372048485997, 2919.1281147618697,\n",
       "        38.71622086, 5502.058523342791, 11.16952458197278],\n",
       "       ...,\n",
       "       [8.405826447527717, 932.9824670593828, 2877.573304216864,\n",
       "        40.47877502, 5260.32185762985, 13.392879121831513],\n",
       "       [8.239607449049426, 951.2825779170732, 2882.3853905260717,\n",
       "        47.95565796, 5244.054888523173, 13.10375519149619],\n",
       "       [8.15488899261671, 971.2098860971196, 2861.211279050305,\n",
       "        47.9231987, 5281.921059587213, 13.024094581994502]], dtype=object)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statistics import mean \n",
    "\n",
    "# load data into readable pandas dataframe of features, X\n",
    "X = pd.read_csv(\"../../../data/sag_data_train.csv\", index_col=\"Time\", parse_dates=True) \n",
    "\n",
    "# convert unformatted into potentially useful numpy format (index is effectively minutes from start)\n",
    "df_train = pd.read_csv(\"../../../data/sag_data_train.csv\", parse_dates=True)\n",
    "\n",
    "Xarr = df_train.to_numpy()\n",
    "\n",
    "# remove time column\n",
    "Xarr = np.delete(Xarr, 0, 1)\n",
    "\n",
    "# rearrange so targets are on right\n",
    "targets = Xarr[:,0:2]\n",
    "Xarr = np.hstack((Xarr,targets))\n",
    "Xarr = np.delete(Xarr, [0,1] , 1)\n",
    "\n",
    "Xarr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though we know there are a couple NAN values, they could be replaced with a sample mean/median value later as to prevent information leakage between training and tets sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8.101647471696031, 873.0851779784513, 2852.5989332409376,\n",
       "        46.21428299, 5405.417261852194, 12.647642714718168],\n",
       "       [8.101204696670724, 872.8418764687068, 2805.952028132609,\n",
       "        42.74369049, 5420.343951850704, 12.682522010182161],\n",
       "       [8.100761922581778, 919.3479448794836, 2837.570360752269,\n",
       "        41.118927, 5401.304281441732, 12.65203169918472],\n",
       "       ...,\n",
       "       [8.4259512904778, 1078.7883803619054, 2864.1288951795455,\n",
       "        44.74312973, 5493.264471579962, 13.606373397687818],\n",
       "       [8.424648682603681, 1060.0573525633752, 2801.392315692223,\n",
       "        39.81977463, 5478.198193228236, 13.556308009611785],\n",
       "       [8.423706855281504, 1045.0455944529467, 2864.534425198114,\n",
       "        42.93088913, 5485.223712860796, 13.534680924206636]], dtype=object)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Due to time and processing power constraints we will cut our data down in size significantly.\n",
    "Bear this in mind if the data contradicts what is being.\n",
    "\"\"\"\n",
    "\n",
    "#Xarr = np.delete(Xarr, slice(0,5000), 0)\n",
    "\n",
    "#Xarr = np.delete(Xarr, slice(15000, Xarr.shape[0]-1), 0)\n",
    "\n",
    "\n",
    "Xarr = Xarr[ 25000:30000, :]\n",
    "Xarr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will format our data into the supervised form via producing a set of lag columns. In order to reduce the training time, while making sure the model fit is looking at a relevant timeframe, we will choose to split the data into 25 minute intervals. The variables outside of this timeframe shouldn't have too much predicting power, so we shouldn't lose too much with regards to that. We will also simultaneously remove the rows at the head of the dataframe that won't have values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output # allows us to keep track of progress\n",
    "\n",
    "# generalised function\n",
    "def timeseries_supervised_conversion(data, lag, lead_time, target_col_indices=[1], interval_length=1, display_column_index=0): # we will use the index to indicate the time interval, so input data needs to be of a regular format\n",
    "     \n",
    "    n_rows = data.shape[0]\n",
    "    n_columns = data.shape[1]\n",
    "    row_n = 0\n",
    "    new_array_row_n = 0\n",
    "    new_data = [] # lists are used for optimisation reasons\n",
    "    \n",
    "    # iterate over rows and columns to append new values\n",
    "    for row in data:\n",
    "        if row_n < lag or ( n_rows-1 - row_n ) < lead_time : # skip rows that return with empty values\n",
    "            row_n += 1\n",
    "            continue\n",
    "            \n",
    "        else:\n",
    "            altered_row = []\n",
    "            altered_row.extend(row)\n",
    "            # iteratively append new targets and features\n",
    "            for L in range(1, lag+1): # All values at every -1 interval are effectively new  features. Skip the first value as it is the original target value\n",
    "                altered_row.extend(data[row_n - L, :])\n",
    "                \n",
    "            for col in target_col_indices:\n",
    "                for lead in range(1, lead_time+1): # every +1 interval of target columns are the targets\n",
    "                    altered_row.append(data[row_n + lead, col])\n",
    "                \n",
    "            new_data.extend(altered_row)\n",
    "            row_n += 1\n",
    "            new_array_row_n += 1 # for reshaping\n",
    "            \n",
    "            clear_output(wait=True) # allows us to keep track of progress\n",
    "            display(new_data[display_column_index], '\\n ... \\n', new_data[len(altered_row)*(new_array_row_n) - 1 + display_column_index])\n",
    "\n",
    "        \n",
    "    new_data = np.array(new_data).reshape(new_array_row_n, n_columns*(lag+1) + lead_time*len(target_col_indices))\n",
    "    \n",
    "    clear_output(wait=True)\n",
    "    display(new_data)\n",
    "    \n",
    "    target_first_index = n_columns*(1+lag)  # for now we can print and input manually , but could be automated via dict() or similar.\n",
    "    print(\"\\n target column indices start at:\", target_first_index )# target columns will be on the right side of the array\n",
    "    \n",
    "    return new_data  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.9908983210870215"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'\\n ... \\n'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "13.152723333455588"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_supervised_full = timeseries_supervised_conversion(Xarr, 20, 5, target_col_indices=[4,5]) \n",
    "display('\\n Shape:', X_supervised_full.shape,'\\n single row looks like: \\n', X_supervised_full[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our problem we are only looking at the 5 minute window. Thus, we can remove the other target columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "target_first_index_here = 126\n",
    "\n",
    "X_supervised = np.delete(X_supervised_full, slice(target_first_index_here + 6*1 + 5, X_supervised_full.shape[1]), 1) # 5 is lead_time, 6 is col_n, and 1 is number of target variables-1\n",
    "X_supervised = np.delete(X_supervised, slice(target_first_index_here + 5, X_supervised_full.shape[1] - 1), 1)\n",
    "X_supervised = np.delete(X_supervised, slice(target_first_index_here, target_first_index_here + 4), 1)\n",
    "display(X_supervised.shape, X_supervised, X_supervised[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This now means that the single target column has an index of the previous first target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "[On timeframe segmentation-validation method]: \n",
    "\n",
    "Since we have so many timeframes to consider and not many NAN values, we can simply remove the whole timeframes wtih them in. This can be done while we split the data into the different timeframes for validation and independent modelling. Here we will start with an arbitrary split of 25 minute timeframes that can be tuned down further in the future if necessary. \n",
    "\n",
    "To do this we define the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split time series data into sets of relevant sizes \n",
    "def time_series_split(integer_time_indexed_array, timeframe, remove_nan_data=False, min_test_size=5, start_time=0, end_time=False): # the units being the smallest time interval indicated by the index\n",
    "    timeframe_n = {}\n",
    "    \n",
    "    if end_time == False:\n",
    "        end_time = integer_time_indexed_array.shape[0]-1\n",
    "        \n",
    "    considered_data = integer_time_indexed_array[start_time:end_time+1,:]\n",
    "    \n",
    "    for i in range( round(considered_data.shape[0]/(timeframe - min_test_size)) ):\n",
    "        timeframe_dataset = considered_data[i*timeframe -  min_test_size*i:(i + 1)*timeframe -  min_test_size*i, :] # time-series data means the last test set of the previous timeframe won't be utilised to train any model so it can be included int the next timeframe's data set\n",
    "        \n",
    "        if ( remove_nan_data == True ) and ( True in pd.isnull(timeframe_dataset) ):\n",
    "            continue\n",
    "        timeframe_n[i] = timeframe_dataset\n",
    "    \n",
    "    return timeframe_n # returns a dictionary of data sets labelled with each timeframes order no.\n",
    "    \n",
    "'''\n",
    "This method is obsolete in this context now. TimeSeriesSplit, from the scikitlearn library is now being used for validation.\n",
    "It is being left in due to potential uses for isolating problematic timeframes in the future.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice at this point that (if remove_nan_data is false) these sets could also be used to sequentially feed data about the parameters of the next model(Metadata) since they are not independent of each other .\n",
    "\n",
    "Run on the data, splitting into a supposed, relevant timeframe. This is due to the intuition that the systems running is likely not affected much by conditions before that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeframe_data_dict = time_series_split(X_supervised, 25, remove_nan_data=True) # split into 25 minute interval timeframes\n",
    "print(len(timeframe_data_dict), '\\n\\n last timeframes dataset:\\n\\n', timeframe_data_dict[len(timeframe_data_dict)-1][0:25]) # check how many timeframes we have"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the removal of certain timeframes, there will be a reduction in direct causality between subsequent timeframes. This only really has an effect on any models where we train the models in a recursive-like fashion with metadata being fed from one timeframe to the next.\n",
    "\n",
    "Also, note here that if we want to produce a table of a given timeframe's dataset we can use the original pandas dataframe and use the datetime values and the datetime indexed dataframe X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check if there is a problem with the overlaps \n",
    "X.loc['2015-12-19 23:25:00':'2015-12-19 23:49:00',:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the rows seem to be accounted for. We now have separated our data into sets that can be used to validate our models and we can move on.\n",
    "\n",
    "From here we will want to develop a framework that will be able to fit various scikitlearn models and test them to check for predictive power. We can do this because of the standardised format of the sklearn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import clone  \n",
    "\n",
    "# fit a single model\n",
    "def fit_model(model, X, y):\n",
    "    # clone the model configuration\n",
    "    current_model = clone(model)\n",
    "    # fit the model\n",
    "    current_model.fit(X, y)\n",
    "    return current_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want a generalised method for implementing the standardised methods of sklearn. First we will produce a dictionary of the initialised models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import clone\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Linear models\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.linear_model import HuberRegressor\n",
    "from sklearn.linear_model import LassoLars\n",
    "from sklearn.linear_model import PassiveAggressiveRegressor\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "# Non-linear models\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.tree import ExtraTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Neural networks\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "# prepare a dictionary of sklearn models\n",
    "def get_models(models=dict()):\n",
    "    param_grid=dict()\n",
    "    # all commented models need further work to apply or are inapplicable\n",
    "    \n",
    "    # Linear models\n",
    "    models['lr'] = dict({'model':  LinearRegression(), 'params':param_grid})\n",
    "    models['lasso'] = dict({'model':  Lasso(), 'params':param_grid})\n",
    "    models['ridge'] = dict({'model':  Ridge(), 'params':param_grid})\n",
    "    models['en'] = dict({'model': ElasticNet(), 'params':param_grid})\n",
    "    # models['huber'] = dict({'model': HuberRegressor(), 'params':param_grid}) \n",
    "    models['llars'] = dict({'model': LassoLars(), 'params':param_grid})\n",
    "    \n",
    "    # Non-linear models\n",
    "    models['knn'] = dict({'model':  KNeighborsRegressor(n_neighbors=7), 'params':param_grid})\n",
    "    models['cart'] = dict({'model':  DecisionTreeRegressor(), 'params':param_grid})\n",
    "    models['extra'] = dict({'model':  ExtraTreeRegressor(), 'params':param_grid})\n",
    "    # models['svmr'] = dict({'model':  SVR(), 'params':param_grid})\n",
    "    ## Ensemble models\n",
    "    n_trees = 100\n",
    "    # models['ada'] = dict({'model':  AdaBoostRegressor(n_estimators=n_trees), 'params':param_grid})\n",
    "    models['bag'] = dict({'model':  BaggingRegressor(n_estimators=n_trees), 'params':param_grid})\n",
    "    models['et'] = dict({'model':  ExtraTreesRegressor(n_estimators=n_trees), 'params':param_grid})\n",
    "    # models['gbm'] = dict({'model':  GradientBoostingRegressor(n_estimators=n_trees), 'params':param_grid})\n",
    "    \n",
    "\n",
    "    ## Random forest\n",
    "    ### define paramaters\n",
    "    n_estimators = [int(x) for x in np.linspace(start = 1, stop = 50, num = 5)]# Number of trees in random forest\n",
    "    max_features = ['auto', 'sqrt'] # Number of features to consider at every split\n",
    "    max_depth = [5,10,100] # Maximum number of levels in tree (depth)\n",
    "    max_depth.append(None)\n",
    "    min_samples_split = [2, 5, 10] # Minimum number of samples required to split a node\n",
    "    min_samples_leaf = [1, 2, 4] # Minimum number of samples required at each leaf node\n",
    "    bootstrap = [True, False] # Method of selecting samples for training each tree\n",
    "    \n",
    "    ### create the param grid\n",
    "    param_grid = dict({'n_estimators': n_estimators,\n",
    "                       'max_features': max_features,\n",
    "                       'max_depth': max_depth,\n",
    "                       'min_samples_split': min_samples_split,\n",
    "                       'min_samples_leaf': min_samples_leaf,\n",
    "                       'bootstrap': bootstrap})\n",
    "    \n",
    "    models['rfr'] = dict({'model':RandomForestRegressor(warm_start=True),\n",
    "                          'params':param_grid})\n",
    "\n",
    "    ## Artificial neural network\n",
    "    \n",
    "    ### define paramaters\n",
    "    hidden_layer_sizes = [(1,),(50,)] # ith no. of neurons in ith hidden layer\n",
    "    activation = ['identity', 'logistic', 'tanh', 'relu'] # Activation function\n",
    "    max_iter = [100,200,300] # Maximumn number of iterations before returning\n",
    "    alpha = [0.00005,0.0005] # L2 penalty (regularisation term) parameter\n",
    "\n",
    "\n",
    "    ### create the param grid\n",
    "    param_grid = dict({'hidden_layer_sizes': hidden_layer_sizes,\n",
    "                       \"activation\": activation,\n",
    "                       'max_iter': max_iter,\n",
    "                       \"alpha\": [0.00005,0.0005]})\n",
    "    \n",
    "    models['mlpr'] = dict({'model':MLPRegressor(max_iter=250, shuffle=False, warm_start=True ),\n",
    "                           'params':param_grid})\n",
    "        \n",
    "    print('Defined %d models' % len(models))\n",
    "    return models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now define a function that can fit each model and return them in a dictionary. We will use TimeSeriesSplit to validate and gather the scores for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "def fit_model(model_key, data, target_col_start, n_splits=5, max_train_size=None):\n",
    "    # print(\"\\n model is\",model_key ,\"\\n\") # test statement\n",
    "    tscv = TimeSeriesSplit(max_train_size=max_train_size, n_splits=n_splits) # 5 split data into a time conserving set of train test splits for CV methods\n",
    "\n",
    "\n",
    "    # prepare dictionary for assigning different models\n",
    "    final_model_scores_dict = dict()\n",
    "    current_model_init = get_models()[model_key]\n",
    "    param_grid = current_model_init['params']\n",
    "    \n",
    "    # test if there are hyperparameters to check\n",
    "    if len(param_grid) != 0:# if there are parameters to optimise we perform a randomised serach cross validation.\n",
    "        \n",
    "        local_model = RandomizedSearchCV(estimator = current_model_init['model'], param_distributions = param_grid, n_iter = 100, cv = tscv.split(data), verbose=2, random_state=42, n_jobs = -1)\n",
    "        local_fit = local_model.fit(data[:,:target_col_start], data[:, target_col_start:])\n",
    "        final_model_scores_dict[ model_key ] = dict({'fitted model':local_fit,\n",
    "                                                      'score': local_fit.best_score_,\n",
    "                                                      'params': local_fit.best_params_})\n",
    "            \n",
    "    else: # otherwise we use a time series split for model validation and get the mean score\n",
    "        local_model = current_model_init['model']\n",
    "        local_split_score = []\n",
    "        for train_index, test_index in tscv.split(data): # split data for validation (this could be donw outside of loop for speed but would require restrucuring)\n",
    "            \n",
    "            sample_data_train, sample_data_test = data[train_index], data[test_index]\n",
    "            X_train, X_test = sample_data_train[:,:target_col_start], data[:,:target_col_start]\n",
    "            y_train, y_test = sample_data_train[:,target_col_start:], data[:,target_col_start:]  \n",
    "\n",
    "            # fit model\n",
    "            local_split_fit = local_model.fit(X_train, y_train)\n",
    "            local_split_score.append(local_split_fit.score(X_test, y_test))\n",
    "            \n",
    "        validated_mean_score = mean(local_split_score)\n",
    "        \n",
    "        \n",
    "        final_model_scores_dict[ model_key ] = dict({'fitted model':local_model.fit(data[:,:target_col_start], data[:,target_col_start:]),\n",
    "                                                      'score': validated_mean_score})\n",
    "        \n",
    "    return final_model_scores_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could tune the hyperparameter $\\Delta T$ for the models by iterating the timeseries_supervised_conversion() function over all the potential 'lag' times. We will save that for later however, and continue with the arbitrarily chosen value of 25. \n",
    "\n",
    "Now we will run the fit_models function over the dictionary of models. We use the joblib library to implement the operation of this function in parallel since this will be the longest process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defined 12 models\n",
      "Defined 12 models\n",
      "model: 1 done \n",
      "\n",
      "Defined 12 models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\louis\\anaconda3\\envs\\py385\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:529: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 360554.2511508576, tolerance: 394.8707976760476\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\louis\\anaconda3\\envs\\py385\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:529: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 620438.0006909769, tolerance: 708.2571501666412\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\louis\\anaconda3\\envs\\py385\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:529: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 906720.1323338819, tolerance: 1347.4582686482327\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\louis\\anaconda3\\envs\\py385\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:529: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1193042.3279363692, tolerance: 1624.9487386469511\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\louis\\anaconda3\\envs\\py385\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:529: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1288600.2796634887, tolerance: 2214.7898698474055\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\louis\\anaconda3\\envs\\py385\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:529: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1009451.8115245984, tolerance: 3002.9415061322115\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\louis\\anaconda3\\envs\\py385\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:529: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1150706.59793683, tolerance: 3187.542676980305\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\louis\\anaconda3\\envs\\py385\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:529: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 906410.3775033876, tolerance: 3392.732693372424\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\louis\\anaconda3\\envs\\py385\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:529: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 961074.7601950737, tolerance: 3767.376915426278\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\louis\\anaconda3\\envs\\py385\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:529: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 147202.79990756605, tolerance: 3924.5346144498803\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: 2 done \n",
      "\n",
      "Defined 12 models\n",
      "model: 3 done \n",
      "\n",
      "Defined 12 models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\louis\\anaconda3\\envs\\py385\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:529: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 410088.1646684804, tolerance: 394.8707976760476\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\louis\\anaconda3\\envs\\py385\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:529: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 699266.3176002667, tolerance: 708.2571501666412\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\louis\\anaconda3\\envs\\py385\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:529: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 943620.3409952867, tolerance: 1347.4582686482327\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\louis\\anaconda3\\envs\\py385\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:529: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1271849.575350706, tolerance: 1624.9487386469511\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\louis\\anaconda3\\envs\\py385\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:529: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.2803017483684016, tolerance: 0.2161418448459238\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\louis\\anaconda3\\envs\\py385\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:529: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1557291.2685525417, tolerance: 2214.7898698474055\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\louis\\anaconda3\\envs\\py385\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:529: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1625601.3363382933, tolerance: 3002.9415061322115\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\louis\\anaconda3\\envs\\py385\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:529: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1702238.0151795126, tolerance: 3187.542676980305\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\louis\\anaconda3\\envs\\py385\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:529: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1664272.6273651407, tolerance: 3392.732693372424\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\louis\\anaconda3\\envs\\py385\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:529: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1577395.445096493, tolerance: 3767.376915426278\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\louis\\anaconda3\\envs\\py385\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:529: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 576694.8976965281, tolerance: 3924.5346144498803\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: 4 done \n",
      "\n",
      "Defined 12 models\n",
      "model: 5 done \n",
      "\n",
      "Defined 12 models\n",
      "model: 6 done \n",
      "\n",
      "Defined 12 models\n",
      "model: 7 done \n",
      "\n",
      "Defined 12 models\n",
      "model: 8 done \n",
      "\n",
      "Defined 12 models\n",
      "model: 9 done \n",
      "\n",
      "Defined 12 models\n",
      "model: 10 done \n",
      "\n",
      "Defined 12 models\n",
      "Fitting 10 folds for each of 100 candidates, totalling 1000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    2.7s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-1)]: Done 632 tasks      | elapsed:  5.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed: 10.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: 11 done \n",
      "\n",
      "Defined 12 models\n",
      "Fitting 10 folds for each of 48 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\louis\\anaconda3\\envs\\py385\\lib\\site-packages\\sklearn\\model_selection\\_search.py:278: UserWarning: The total space of parameters 48 is smaller than n_iter=100. Running 48 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    4.2s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:   23.0s\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 480 out of 480 | elapsed:  2.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: 12 done \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from joblib import Parallel, delayed \n",
    "\n",
    "target_first_index_here = 126\n",
    "model_dict = dict()\n",
    "input_X = X_supervised.astype(np.float64) # Data is recommended to be cast as floats\n",
    "\n",
    "#def superdict_models(model_dict=dict()) # part of obsolete method but this comment is kept for personal reminder\n",
    "# parallelised model fitting\n",
    "\"\"\"\n",
    "model_dict.update(Parallel(n_jobs=-1, verbose=51)(delayed(fit_model)(model_key, X_supervised, target_first_index_here, n_splits=10) for model_key in get_models()))\n",
    "\n",
    "display(model_dict)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Alternate method - We will use this use the n_jobs=-1 parameter on the RandomSearchCV model fit since that is a \n",
    "# much larger part of how long the whole process will take. It is also used on the more computing-power intensive models.\n",
    "n=0\n",
    "for model_key in get_models():\n",
    "    n+=1\n",
    "    model_dict.update(fit_model(model_key, input_X, target_first_index_here, n_splits=10))\n",
    "    print(\"model:\",n,\"done \\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if we check for the best score among all these models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8701081748630743, 0.6894230760121487, 0.8731181745356429, 0.6943590460967181, 0.09653563192339401, 0.5207602906515384, 0.6811964848018733, 0.6996203127999078, 0.7554822237390272, 0.7617553696457516, 0.32309851732757977, -18.747654383274853] \n",
      "  ['lr', 'lasso', 'ridge', 'en', 'llars', 'knn', 'cart', 'extra', 'bag', 'et', 'rfr', 'mlpr']\n"
     ]
    }
   ],
   "source": [
    "score_list = []\n",
    "model_list = []\n",
    "for model in model_dict :\n",
    "    score_list.append(model_dict[model]['score'])\n",
    "    model_list.append(model)\n",
    "\n",
    "print(score_list, '\\n ', model_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find that 'lr' is the current most accurate model. This is somewhat expected in that the data does behave somewhat linearly most of the time as we saw on the large timescale size. Though, we did expect the random forest or ANN to perform best, especially given the sample characteristics being constrained within limits as they were. However, we did have to cut down a very large amount of the input data in exchange for time; though, we would expect the data given to be enough to get a better fit at least than a straight line (the negative score for the random forest). \n",
    "\n",
    "Another thing that should be considered is that they were both fit along the same branch along with the RandomSearchCV, so there is potential for a bug to be contained in that branch (future ISSUE).\n",
    "\n",
    "Thus, using this best-scoring model to compare the performance against the baseline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5377.29412529,   12.23562592,    7.86798698, ...,   48.24573517,\n",
       "          42.45809174,   42.72625351],\n",
       "       [5425.89384904,   12.27434863,    8.07584687, ...,   42.45809174,\n",
       "          42.72625351,   46.10914993],\n",
       "       [5427.7451188 ,   13.15642791,    8.43865228, ...,   42.72625351,\n",
       "          46.10914993,   43.89574051],\n",
       "       ...,\n",
       "       [5234.43787441,   13.9514216 ,    8.61804365, ...,   40.85377502,\n",
       "          40.89812469,   41.10262299],\n",
       "       [5205.51158586,   13.8548517 ,    8.62721867, ...,   40.89812469,\n",
       "          41.10262299,   35.96046448],\n",
       "       [5222.72087902,   13.8786745 ,    8.61210701, ...,   41.10262299,\n",
       "          35.96046448,   41.89967728]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " target column indices start at: 126\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# load in test data\n",
    "df_test = pd.read_csv(\"../../../data/sag_data_test.csv\", parse_dates=True)\n",
    "test_arr = df_test.to_numpy()\n",
    "test_supervised_pre = np.delete(test_arr, 0, 1).astype(np.float64) # delete time and convert to recommended type\n",
    "\n",
    "# rearrange so targets are on right\n",
    "test_targets = test_arr[:,0:2]\n",
    "test_arr = np.hstack((test_arr,test_targets))\n",
    "test_arr = np.delete(test_arr, [0,1] , 1)\n",
    "\n",
    "\n",
    "# set up conditions\n",
    "target_first_index_here = 126  # obtained earlier and still manually inputted for now\n",
    "best_model = 'rfr'\n",
    "# process data into input format\n",
    "test_supervised = timeseries_supervised_conversion(test_supervised_pre, 20, 5, target_col_indices=[4,5]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this processed data we now feed it into the model for prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2599.368359\n",
       "1      33.637177\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "test_X = test_supervised[:,:target_first_index_here]\n",
    "test_predictions = model_dict[best_model]['fitted model'].predict(test_X)\n",
    "test_actual = test_supervised[:,target_first_index_here + 4:target_first_index_here + 6]\n",
    "errors = test_predictions - test_actual\n",
    "\n",
    "error_df = pd.DataFrame(errors) # 0 is bearing pressure and 1, power draw\n",
    "rmse = (error_df**2).mean()**0.5\n",
    "display(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, without even looking at the baseline we can see something is wrong here since the values are so high. This is likely a bug.\n",
    "\n",
    "Plotting the errors with same axis as baseline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "error_df[0].hist(bins=31, range=(-200, 200), edgecolor=\"k\").set_title('Bearing Pressure')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors are too large for these axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "error_df[1].hist(bins=31, range=(-1.5, 1.5), edgecolor=\"k\").set_title('Power Draw')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the current best performing model is significantly worse than the baseline. Though this is not  completely unexpected due to the fact that the baseline effectively accounts for the oscilating a lot better rather than trying to fit the oscillations onto a line, how badly the model seems to be performing is likely a bug in the display or development. \n",
    "\n",
    "Furthermore, we would expect the \"smarter\" models to perform better, or at least as well. Perhaps this would be the case if all the data were used and parameters were fine tuned more. Though, with how badly they scored, a bug somewhere in the RandomSearchCV section of fit_model. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
